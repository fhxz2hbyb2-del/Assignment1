{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2da23801-0772-4fa5-be8e-d0ebdba0b6f2",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to scrape book cover images and titles from [Books to Scrape](http://books.toscrape.com) and save the data into a CSV. The CSV will have two columns: \n",
    "→ URL of the book cover image  \n",
    "→ the metadata (title of the book)\n",
    "\n",
    "We focus only on **5-star rated books**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6208dd-e145-4357-acb0-b9579efd1227",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f1a5a8-a229-4adf-b2dd-4657f2e38404",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"http://books.toscrape.com/catalogue/page-{}.html\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3116206-502d-4799-b00e-d0ba75580cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "books = []\n",
    "page = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1118e315-94f5-467a-9a93-c876f214d6a8",
   "metadata": {},
   "source": [
    "I will use a loop to access all pages on the website, since it now will only do page 1. To see its 'current scraping status', I create a little program. I loop through each page until there are no more pages (HTTP status code not 200 or no books found). For each book on a page:\n",
    "\n",
    "1. Check the star rating and keep only 5-star books.  \n",
    "2. Extract the title and clean it using UTF-8 encoding and regex to remove weird characters.  \n",
    "3. Extract the relative image URL and convert it to an absolute URL.  \n",
    "4. Store the image URL and the cleaned title in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b502e8-626a-4d76-8b2f-7464fcc07313",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    url = base_url.format(page)\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        break  # this makes the program stop when a page does not exist (so on the last page)\n",
    "    \n",
    "    response.encoding = 'utf-8'\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    book_elements = soup.select(\".product_pod\")\n",
    "    if not book_elements:\n",
    "        break  # stop if there are no more books to scrape\n",
    "    \n",
    "    for book in book_elements:\n",
    "        rating_class = book.select_one(\".star-rating\")['class']\n",
    "        if \"Five\" in rating_class:\n",
    "            title = book.h3.a['title']\n",
    "            title = title.encode('utf-8', errors='ignore').decode('utf-8')\n",
    "            title = re.sub(r'[^\\x20-\\x7E]+', '', title).strip()\n",
    "            \n",
    "            img_relative_url = book.select_one(\"img\")['src']\n",
    "            img_url = \"http://books.toscrape.com/\" + img_relative_url.replace(\"../\", \"\")\n",
    "            \n",
    "            books.append({\n",
    "                \"content\": img_url,\n",
    "                \"metadata\": title\n",
    "            })\n",
    "    \n",
    "    print(f\"Scraped page {page}...\") #shows the scraping status\n",
    "    page += 1 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62dcf62e-9d57-4e55-9d42-72d52f0ebfc0",
   "metadata": {},
   "source": [
    "Once all 5-star books have been collected, we save them into a CSV file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c904905-a6e8-4dc9-8c99-4b68e2a7ef76",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_filename = \"books_5stars_covers.csv\"\n",
    "\n",
    "with open(csv_filename, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=[\"content\", \"metadata\"])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(books)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb553e4-d240-4ee3-9fa4-346605ff7ea2",
   "metadata": {},
   "source": [
    "# Notes\n",
    "→ The `re.sub(r'[^\\x20-\\x7E]+', '', title)` step removes any weird characters that can appear in book titles.  \n",
    "→ Encoding is enforced as UTF-8 to avoid issues with characters like `â`\n",
    "→ The script automatically loops through all pages, so it scales to the entire website.  \n",
    "→ The CSV is ready for corpus use, with image URLs in the `content` column and the book title as metadata."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
