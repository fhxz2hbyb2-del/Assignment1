{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "808d82d9",
   "metadata": {},
   "source": [
    "# Text Preprocessing and Annotation Notebook\n",
    "This notebook contains all the code I used for preprocessing, annotating, and saving the a corpus of all Pokémon and their descriptions. \n",
    "\n",
    "## 1. Getting my data\n",
    "   \n",
    "```\n",
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "pokemon_data = []\n",
    "\n",
    "for i in range(1, 1026):\n",
    "    print(f\"Getting Pokémon {i}...\") #just to see what the downloading status is \n",
    "    \n",
    "    for attempt in range(3):\n",
    "        p_resp = requests.get(f\"https://pokeapi.co/api/v2/pokemon/{i}\")\n",
    "        if p_resp.status_code == 200:\n",
    "            break\n",
    "        print(f\"Retry {attempt+1} for Pokémon {i} (status {p_resp.status_code})\")\n",
    "        time.sleep(1)\n",
    "    else:\n",
    "        print(f\"Skipping Pokémon {i} after 3 failed attempts\")     #retry to get info up to 3 times\n",
    "        continue\n",
    "\n",
    "    p = p_resp.json()\n",
    "\n",
    "    for attempt in range(3):\n",
    "        s_resp = requests.get(f\"https://pokeapi.co/api/v2/pokemon-species/{i}\")\n",
    "        if s_resp.status_code == 200:\n",
    "            break\n",
    "        print(f\"Retry {attempt+1} for species {i} (status {s_resp.status_code})\")\n",
    "        time.sleep(1)\n",
    "    else:\n",
    "        print(f\"Skipping species {i} after 3 failed attempts\")\n",
    "        continue\n",
    "\n",
    "    s = s_resp.json()\n",
    "\n",
    "    english_entries = [\n",
    "        e['flavor_text'].replace(\"\\n\", \" \").replace(\"\\x0c\", \" \")\n",
    "        for e in s['flavor_text_entries']\n",
    "        if e['language']['name'] == 'en'\n",
    "    ]\n",
    "    description = english_entries[0] if english_entries else \"\"\n",
    "\n",
    "    filename = f\"{i:04d}_{p['name']}.txt\"\n",
    "    with open(os.path.join(\"data\", filename), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(description)\n",
    "\n",
    "    pokemon_data.append({\n",
    "        \"id\": i,\n",
    "        \"name\": p['name'],\n",
    "        \"types\": [t['type']['name'] for t in p['types']],\n",
    "        \"description\": description,\n",
    "        \"filename\": filename\n",
    "    })\n",
    "\n",
    "    time.sleep(0.3)  # delay to avoid overload\n",
    "\n",
    "df = pd.DataFrame(pokemon_data)\n",
    "df.to_csv(\"pokedex.csv\", index=False)\n",
    "print(\"Done! CSV and .txt files saved in 'data/' folder.\")\n",
    "\n",
    "```\n",
    "\n",
    "## 2. Imports and preprocessing \n",
    "\n",
    "```\n",
    ">>> import spacy\n",
    ">>> nlp = spacy.load(\"en_core_web_sm\")\n",
    ">>> import os\n",
    ">>> from spacy import displacy\n",
    ">>> from IPython.display import display, HTML\n",
    ">>> import spacy\n",
    ">>> import pandas as pd\n",
    ">>> pd.options.mode.chained_assignment = None\n",
    ">>> import plotly.express as px\n",
    ">>> texts = []\n",
    ">>> file_names = []\n",
    "\n",
    ">>> for _file_name in os.listdir('pokedata'):\n",
    "...     if _file_name.endswith('.txt'):\n",
    "...             texts.append(open('pokedata' + '/' + _file_name, 'r', encoding='utf-8').read())\n",
    "...             file_names.append(_file_name)\n",
    "... \n",
    "\n",
    ">>> metadata_df = pd.read_csv('pokemetadata.csv', delimiter=';')\n",
    ">>> final_poke_df = metadata_df.merge(pokedata_df,on='filename')\n",
    "\n",
    ">>> def preprocess_text(text):\n",
    "...     text = text.lower()\n",
    "...     text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "...     text = re.sub(r'\\S+@\\S+', '', text)\n",
    "...     text = re.sub(r'\\d{2,4}[-.\\s]?\\d{3,4}[-.\\s]?\\d{3,4}', '', text)\n",
    "...     text = re.sub(r'[^\\w\\s]', '', text)\n",
    "...     text = re.sub(r'\\s+', ' ', text).strip()\n",
    "...     return text\n",
    "... \n",
    "\n",
    ">>> final_poke_df['clean_text'] = final_poke_df['text'].apply(preprocess_text)\n",
    "\n",
    "```\n",
    "\n",
    "## 3. Adding Annotations\n",
    "\n",
    "```\n",
    ">>> nlp = spacy.load('en_core_web_sm')\n",
    ">>> print(nlp.pipe_names)\n",
    "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
    "\n",
    ">>> def process_text(text):\n",
    "...      return nlp(text)\n",
    "... \n",
    ">>> final_poke_df['doc'] = final_poke_df['clean_text'].apply(process_text)\n",
    "\n",
    ">>> def get_token(doc):\n",
    "...      return [(token.text) for token in doc]\n",
    "... \n",
    ">>> final_poke_df['tokens'] = final_poke_df['doc'].apply(get_token)\n",
    ">>> def get_lemma(doc):\n",
    "...     return [(token.lemma_) for token in doc]\n",
    "... \n",
    ">>> final_poke_df['lemmas'] = final_poke_df['doc'].apply(get_lemma)\n",
    ">>> def get_pos(doc):\n",
    "...     return [(token.pos_, token.tag_) for token in doc]\n",
    "... \n",
    ">>> final_poke_df['POS'] = final_poke_df['doc'].apply(get_pos)\n",
    ">>> def extract_proper_nouns(doc):\n",
    "...     return [token.text for token in doc if token.pos_ == 'PROPN']\n",
    "... \n",
    ">>> final_poke_df['proper_nouns'] = final_poke_df['doc'].apply(extract_proper_nouns)\n",
    ">>> final_poke_df = final_poke_df.drop(['clean_text'], axis=1)\n",
    ">>> final_poke_df.to_csv('pokedata_preprocessed_annotated.csv')\n",
    ">>> exit()\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
